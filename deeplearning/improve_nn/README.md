# 神经网络优化
在构建神经网络模型的过程中，超参数的选取是十分重要的，隐藏层个数、神经元个数、学习速率、激活函数等等因素都会影响模型最终的表现，我们需要不断地实验和验证来决定理想的超参数设定。

除此之外，在构建深度学习模型的过程中还有很多技巧可以使得训练更容易、模型的表现更好。



## 数据集划分

在传统的机器学习模型中，一般采用80/20的方式划分训练集和测试集，或者60/20/20的方式划分训练集/验证集/测试集。

在深度学习模型中，因为需要投喂大量的数据，甚至能到百万、千万的数据量，这时候的验证集和测试集就不需要占比太多，比如95/2.25/2.5的占比方式就可以。

## 正则化

在《机器学习》笔记中提到

- 当偏差较大时，说明模型**欠拟合**，考虑增加模型的复杂度，比如使用更复杂的算法，增加神经网络的层数和神经元等
- 当方差较大时，说明模型**过拟合**，考虑增加更多数据进行训练，或者使用正则化方法等。

而因为神经网络一般具有很复杂的结构，训练出的模型很容易会过拟合，除了增加更多训练数据外，正则化是一种十分有效的方法。

### L1/L2正则化

在一般的机器学习算法，比如逻辑回归、线性回归中，我们使用L1正则化、L2正则化来减缓过拟合现象，在神经网络中我们也能使用这种正则化。

神经网络模型的代价函数为：

$J\left(w^{[1]}, b^{[1]}, \ldots, w^{[l]}, b^{[l]}\right)=\frac{1}{m}\sum_{i=1}^m L(\hat{y}^{(i)},y^{(i)})$

加入L2正则项后变为：

$J\left(w^{[1]}, b^{[1]}, \ldots, w^{[l]}, b^{[l]}\right)=\frac{1}{m}\sum_{i=1}^m L(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}\sum_{l=1}^L||w^{[l]}||^2_F$

其中$||w^{[l]}||^2_F=\sum_{i=1}^{n^{[l-1]}}\sum_{j=1}^{n^{[l]}}(w_{ij}^{[l]})^2$

在使用梯度下降更新参数的时候也需要减掉这个正则项。

### Dropout正则化

#### 基本思想

在神经元输出到下一层时，随机地让下一层的一些神经元工作，一部分不工作，在每一次训练时随机使用残缺的神经网络，这样的方法能在一定程度上抵抗过拟合现象。

![image-20220221155840530](images/README/image-20220221155840530.png)

比如上图中，在一次训练时我们随机drop out掉隐藏层中的第一个神经元，这样与它就不会继续向下传播。

具体表现就是，原来在输入下一层的时候，在进行$z^{[l+1]}=W^{[l+1]}a^{[l]}+b^{[l+1]}$计算之前，先将上一层的输出$a^{[l]}$与一个服从伯努利分布的变量$r^{[l]}$相乘，给定一个概率（比如0.9、0.5等 ），得到一个新的$\bar a^{[l]}$，这样这层中的每个神经元就会有一定的概率不会向下传播。 

![image-20220221160119334](images/README/image-20220221160119334.png)

#### 形象例子：

如果输入的是40X40像素的图片作为数据，那么一共有40X40X3=4800个输入特征。

构建一个神经网络，在输入层传播到第一个隐藏层的过程中使用Dropout，设定保留概率为0.8。

那么在每一次训练时，这4800个特征每一个都有可能被暂时隐藏，最后可能只剩下3800个特征，也就相当于每次更新，神经网络都只接受了一张残缺的图片，有一些像素块是被遮挡住的。

#### 实现——Inverted Dropout

假设神经网络中有3层隐藏层，我们仅对隐藏层使用Dropout

```python
A0 = '...' # 第一层隐藏层的输入，从前向传播中获得
A1 = '...' # 第二层隐藏层的输入，从前向传播中获得
A2 = '...' # 第三层隐藏层的输入，从前向传播中获得
prob = 0.8 # 设置保留神经元的概率，有20%的神经元会被隐藏
for i in range(1, L + 1): 
    # L为隐藏层个数，我们对每一层隐藏层都使用Dropout
    'd' + str(i) = np.random.rand(A + str(i).shape[0],A + str(i).shape[1]) < prob
    # 创建向量d1, d2, d3，它们的形状分别等于A1, A2, A3，且里面每个元素值为0或1，取决于创建的0-1之间的随机数是否小于0.8
    A + str(i) = np.multiply(A + str(i), d + str(i))
    # 将每一层的A与d相乘得到新的A，A中的一些元素的值乘以0得到0，其它的元素乘以1大小不变，达成了随机隐藏20%神经元的目的
```

注意，在对测试集进行测试的时候，不要继续使用Dropout方法，而应该用原全连接层的所有参数进行前向传播。

#### 为什么行得通？

**不让神经网络过度依靠任何一个特征**

​	在通过面积、所在城市、卧室数量、附近是否有学校四个特征预测房价的回归任务中，使用神经网络，在输入层到第一层隐藏层中可能会一直给面积这一特征赋予很高的权重，导致最终的模型过度依赖面积这一因素而不太考虑其他因素。

​	在输入层使用Dropout后，随机隐藏输入的特征（神经元），某些时候就会隐藏到面积这一特征，这时神经网络只能依靠其他三个特征，就不得不给它们赋予更高的权重，最后的模型得到的权重得到分散，三个特征都有考虑，用在新的各种各样的数据上的表现就会更好。

### Data Augmentation 数据增强 

神经网络的训练需要大量的数据，但在现实里搜集数据是一件耗时耗力甚至耗资金的事，因此通过一些方法让已有的数据发挥更大的作用，能够给神经网络带来很大的提升，数据增强就是这么一种方法。

在计算机视觉相关的问题中，我们可以对已有的图片进行各种改动来获取更多的数据。比如在识别猫还是狗的二分类问题中，我们可以将图片进行翻转、缩放、裁剪等操作，得到”新“的图片，虽然人一眼能看出这是同一种图片，但对于计算机来说，输入的像素特征已经完全被打乱，这是一种很有效的增加数据的方法。

![image-20220221173620381](images/README/image-20220221173620381.png)

#### 实现

```python
import cv2 # 使用open-cv库来实现图片的变形
# 读取图片
img = cv2.imread('/cat.jpg') 
# 将图片垂直翻转
img_flip_along_x = cv2.flip(img, 0) 
# 将图片水平翻转
img_flip_along_x = cv2.flip(img, 1) 
# 将图片顺时针180度翻转
img_flip_along_xy = cv2.flip(img, -1)

# 将图片裁剪 高度上保留100-300像素，宽度上保留150-350像素
img_cut=img[100:300,150:350] # 

# 将图片缩放为200x200的大小
img_200x200 = cv2.resize(img, (200, 200)) 
```

### Early stopping 早停法

#### 原理

过拟合的表现是神经网络在训练集上表现很好，但在验证集或测试集上的表现不好。实际上，在训练的过程中，随着迭代次数的增加，尽管模型在训练集上的损失在逐步降低，但其在验证集/测试集上的损失可能是反复增减的，在某一次迭代时就已经达到最小损失了，因此跑完全部的迭代次数可能会使得模型在新数据上的表现变差。

![Cross-validation-Early-stopping-Principe-2000](images/README/Cross-validation-Early-stopping-Principe-2000.png)

source: https://www.researchgate.net/figure/Cross-validation-Early-stopping-Principe-2000_fig4_228469923

早停法的原理就是在训练的过程中计算模型在验证集上的表现，只要模型在验证集上的表现开始下降的时候就停止训练。

#### 为什么行得通？

在训练的过程中，随着迭代次数的增加，权重会更新到越来越大的值，这样的权重会在训练集上表现得越来越好，但遇到未见过的数据时就会表现差，因此在中间点就停止训练的话可以得到中等大小的权重值，更能拟合未见过的数据。

#### 停止标准

1. 第一类停止标准

​	当模型在验证集上的损失和在训练集上的损失相差过大的时候说明模型过拟合了，因此控制两个损失之间的差距就是第一种控制方法，假设$E_train(t)$是在迭代次数$t$时在训练集上取得的误差

### 归一化 Normalisation

#### 为什么要使用归一化

1. 减小对特征差异对训练结果的影响

   对于一些机器学习/深度学习任务来说，输入数据的特征的范围相差较大，比如预测房价的回归任务中，输入特征有房屋面积和到市中心距离，面积的数值区间为几十到几百（平方米），而到市中心距离的数值区间为几百到几万（米）。因为输入特征范围的巨大差距使得与其相对应的权重参数会有很大的差距，最终训练得到的结果表明到市中心距离对于房价的影响要大得多，而实际上并不是这样，结果就造成了模型的表现不好。

   将输入特征归一化后，就能比较有效地避免这个问题，因为特征的取值都相差不大，不会出现一个或几个特征“独大”的情况。

2. 加快模型训练的收敛速度

​	因为输入特征范围的巨大差距使得与其相对应的权重和偏置参数也会有很大的差距。

​	代价$J$与权重$w$和偏置$b$的关系图就会呈现下图的样子：

![image-20220222002416039](images/README/image-20220222002416039.png)

​	俯视图如下图：

![image-20220222002108521](images/README/image-20220222002108521.png)

​	在对参数进行更新时，梯度下降算法会沿着参数的梯度（导数）方向进行更新，也就是垂直于当前点的方向进行移动，直到最终到达，其更新过程如下图：

![image-20220222002545951](images/README/image-20220222002545951.png)

可以观察到，因为路线非常陡峭，所以需要很多很多步才能抵达代价最小点。



归一化就是将输入的特征控制在同的一个区间内，比如进行缩放让它们的值都在0-1范围内，处理之后代价$J$与权重$w$和偏置$b$的关系图就会呈现下图的样子：

![image-20220222002900059](images/README/image-20220222002900059.png)

俯视图以及更新路线：

![image-20220222002919646](images/README/image-20220222002919646.png)

![image-20220222002954645](images/README/image-20220222002954645.png)

source: Deep Learning Coursera by Andrew Ng

可以观察到，进行归一化后更新的速率大幅提升，模型更容易收敛，减少了训练时间。

#### 归一化的方法

1. z-score （标准化 standardization）
